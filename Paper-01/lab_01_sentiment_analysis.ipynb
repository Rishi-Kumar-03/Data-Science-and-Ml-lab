{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "afy_h4TtZZXU",
        "outputId": "9cf3b74b-f240-448c-9c95-538662da737a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded 1400 docs (700 pos / 700 neg)\n",
            " Dataset version: Polarity v0.9/v1.0 (EMNLP 2002) – 700 pos + 700 neg\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install -q scikit-learn pandas\n",
        "\n",
        "import os, re, numpy as np, pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "\\\n",
        "url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_cleaned.zip\"\n",
        "zip_path = \"mix20_rand700_tokens_cleaned.zip\"\n",
        "\n",
        "\n",
        "if not os.path.exists(zip_path):\n",
        "    print(\"Downloading dataset...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "\n",
        "data_folder = os.path.splitext(zip_path)[0]\n",
        "if not os.path.exists(data_folder):\n",
        "    print(\" Unzipping dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_folder)\n",
        "\n",
        "\n",
        "if os.path.exists(os.path.join(data_folder, \"tokens\", \"pos\")):\n",
        "    pos_dir = os.path.join(data_folder, \"tokens\", \"pos\")\n",
        "    neg_dir = os.path.join(data_folder, \"tokens\", \"neg\")\n",
        "    version = \"Polarity v0.9/v1.0 (EMNLP 2002) – 700 pos + 700 neg\"\n",
        "elif os.path.exists(os.path.join(data_folder, \"txt_sentoken\", \"pos\")):\n",
        "    pos_dir = os.path.join(data_folder, \"txt_sentoken\", \"pos\")\n",
        "    neg_dir = os.path.join(data_folder, \"txt_sentoken\", \"neg\")\n",
        "    version = \"Polarity v2.0 (ACL 2004) – 1000 pos + 1000 neg\"\n",
        "else:\n",
        "    raise FileNotFoundError(\"Could not detect dataset version.\")\n",
        "\n",
        "\n",
        "def load_data(pos_dir, neg_dir):\n",
        "    texts, labels = [], []\n",
        "    for folder, label in [(pos_dir, 1), (neg_dir, 0)]:\n",
        "        for fname in os.listdir(folder):\n",
        "            if not fname.endswith(\".txt\"):\n",
        "                continue\n",
        "            with open(os.path.join(folder, fname), encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                txt = f.read()\n",
        "            # Remove rating hints like \"10/10\", \"****\"\n",
        "            txt = re.sub(r\"\\d+/\\d+|\\*+\", \"\", txt)\n",
        "            texts.append(txt)\n",
        "            labels.append(label)\n",
        "    return texts, np.array(labels)\n",
        "\n",
        "\n",
        "texts, labels = load_data(pos_dir, neg_dir)\n",
        "print(f\"\\nLoaded {len(texts)} docs ({sum(labels)} pos / {len(labels)-sum(labels)} neg)\")\n",
        "print(f\" Dataset version: {version}\\n\")\n",
        "\n",
        "\n",
        "def evaluate_model(clf, X, y):\n",
        "    return cross_val_score(clf, X, y, cv=3).mean() * 100\n",
        "\n",
        "\n",
        "results = []\n",
        "token_pattern = r\"(?u)\\b\\w+\\b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mekBgf7palgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffa9150-7525-4738-9666-7ee9539561fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Final Results Table:\n",
            "\n",
            "               Features  #Features        NB        ME       SVM\n",
            "    (1) unigrams (freq)      12960 78.713856       NaN 77.929162\n",
            "(2) unigrams (presence)      12960 81.356511 83.142483 81.642634\n",
            "   (3) unigrams+bigrams      24462 81.713552 82.571155 80.999776\n",
            "       (4) bigrams only      15825 79.285948 77.857324 75.572016\n",
            "    (5) adjectives only       1937 76.069975 73.356095 69.856142\n",
            "  (6) top 2633 unigrams       2633 80.855183 80.712734 78.427273\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "# (1) Unigrams (frequency)\n",
        "vectorizer = CountVectorizer(binary=False, token_pattern=token_pattern, min_df=4)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "results.append([\"(1) unigrams (freq)\", X.shape[1],\n",
        "                evaluate_model(MultinomialNB(), X, labels),\n",
        "                None,\n",
        "                evaluate_model(LinearSVC(max_iter=5000), X, labels)])\n",
        "\n",
        "# (2) Unigrams (presence)\n",
        "vectorizer = CountVectorizer(binary=True, token_pattern=token_pattern, min_df=4)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "results.append([\"(2) unigrams (presence)\", X.shape[1],\n",
        "                evaluate_model(MultinomialNB(), X, labels),\n",
        "                evaluate_model(LogisticRegression(max_iter=1000), X, labels),\n",
        "                evaluate_model(LinearSVC(max_iter=5000), X, labels)])\n",
        "\n",
        "# (3) Unigrams + Bigrams\n",
        "vectorizer = CountVectorizer(binary=True, ngram_range=(1,2),\n",
        "                             token_pattern=token_pattern, min_df=7)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "results.append([\"(3) unigrams+bigrams\", X.shape[1],\n",
        "                evaluate_model(MultinomialNB(), X, labels),\n",
        "                evaluate_model(LogisticRegression(max_iter=1000), X, labels),\n",
        "                evaluate_model(LinearSVC(max_iter=5000), X, labels)])\n",
        "\n",
        "# (4) Bigrams only\n",
        "vectorizer = CountVectorizer(binary=True, ngram_range=(2,2),\n",
        "                             token_pattern=token_pattern, min_df=7)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "results.append([\"(4) bigrams only\", X.shape[1],\n",
        "                evaluate_model(MultinomialNB(), X, labels),\n",
        "                evaluate_model(LogisticRegression(max_iter=1000), X, labels),\n",
        "                evaluate_model(LinearSVC(max_iter=5000), X, labels)])\n",
        "\n",
        "\n",
        "def adj_tokenizer(text):\n",
        "    tokens = re.findall(r\"\\b\\w+\\b\", text)\n",
        "    return [w for w in tokens if re.match(r\".*ly$|.*ous$|.*ful$|.*able$|.*ive$|.*less$|.*ic$|.*al$|.*est$|.*er$|good|bad|great|awful|excellent|poor\", w.lower())]\n",
        "\n",
        "vectorizer = CountVectorizer(tokenizer=adj_tokenizer, binary=True, min_df=4)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "results.append([\"(5) adjectives only\", X.shape[1],\n",
        "                evaluate_model(MultinomialNB(), X, labels),\n",
        "                evaluate_model(LogisticRegression(max_iter=1000), X, labels),\n",
        "                evaluate_model(LinearSVC(max_iter=5000), X, labels)])\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True, token_pattern=token_pattern, max_features=2633)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "results.append([\"(6) top 2633 unigrams\", X.shape[1],\n",
        "                evaluate_model(MultinomialNB(), X, labels),\n",
        "                evaluate_model(LogisticRegression(max_iter=1000), X, labels),\n",
        "                evaluate_model(LinearSVC(max_iter=5000), X, labels)])\n",
        "\n",
        "\n",
        "df = pd.DataFrame(results, columns=[\"Features\",\"#Features\",\"NB\",\"ME\",\"SVM\"])\n",
        "df = df.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "print(\"\\n Final Results Table:\\n\")\n",
        "print(df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YWe3hQ_rZdHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}