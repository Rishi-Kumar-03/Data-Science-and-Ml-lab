{"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import random\n","from collections import defaultdict\n","import re\n","from sklearn.feature_extraction.text import CountVectorizer\n","from datasets import load_dataset\n","# 1 LOAD YELP REVIEWS DATASET\n","print(\"Loading Yelp dataset...\")\n","dataset = load_dataset(\"yelp_review_full\", split=\"train[:10%]\")  # use 10% for speed\n","# Convert to DataFrame\n","df = pd.DataFrame(dataset)\n","# Simulate document (business) IDs and user IDs\n","df[\"asin\"] = [\"business_\" + str(i % 4000) for i in range(len(df))]  # ~4k businesses\n","df[\"reviewText\"] = df[\"text\"]\n","df[\"reviewerID\"] = [\"user_\" + str(i % 6000) for i in range(len(df))]  # ~6k users\n","df = df[[\"reviewerID\", \"asin\", \"reviewText\"]].dropna()\n","print(\"✅ Yelp dataset loaded:\", len(df), \"reviews\")\n","\n","# 2 FILTER USERS AND DOCUMENTS\n","min_reviews_user = 2\n","min_reviews_doc = 2\n","user_counts = df['reviewerID'].value_counts()\n","doc_counts = df['asin'].value_counts()\n","\n","users_keep = user_counts[user_counts >= min_reviews_user].index\n","docs_keep  = doc_counts[doc_counts >= min_reviews_doc].index\n","\n","df2 = df[df['reviewerID'].isin(users_keep) & df['asin'].isin(docs_keep)].copy()\n","print(\"Filtered reviews:\", len(df2))\n","\n","# Relabel indices\n","user2idx = {u: i for i, u in enumerate(df2['reviewerID'].unique())}\n","doc2idx  = {d: i for i, d in enumerate(df2['asin'].unique())}\n","\n","df2['u_idx'] = df2['reviewerID'].map(user2idx)\n","df2['d_idx'] = df2['asin'].map(doc2idx)\n","\n","U = len(user2idx)\n","D = len(doc2idx)\n","print(\"Num users:\", U, \"Num documents:\", D)\n","\n","# =====================================\n","# 3 TEXT PREPROCESSING & VOCABULARY\n","# =====================================\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text) # Remove numbers and punctuation\n","    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n","    return text.split()\n","\n","all_texts = df2['reviewText'].tolist()\n","vectorizer = CountVectorizer(max_features=2000, stop_words='english', tokenizer=preprocess)\n","X_counts = vectorizer.fit_transform(all_texts)\n","vocab = vectorizer.get_feature_names_out()\n","W = len(vocab)\n","print(\"Vocabulary size:\", W)\n","\n","word2idx = {w: i for i, w in enumerate(vocab)}\n","\n","# =====================================\n","# 4 BUILD (u, d, w) TRIPLETS\n","# =====================================\n","triplet_counts = defaultdict(int)\n","for _, row in df2.iterrows():\n","    u = row['u_idx']\n","    d = row['d_idx']\n","    tokens = preprocess(row['reviewText'])\n","    for t in tokens:\n","        if t in word2idx:\n","            w = word2idx[t]\n","            triplet_counts[(u, d, w)] += 1\n","\n","triplets = [(u, d, w, c) for (u, d, w), c in triplet_counts.items() if c > 0]\n","print(\"Num non-zero triplets:\", len(triplets))\n","\n","random.shuffle(triplets)\n","split = int(0.9 * len(triplets))\n","train_triplets = triplets[:split]\n","test_triplets  = triplets[split:]\n","print(\"Train size:\", len(train_triplets), \"Test size:\", len(test_triplets))\n","\n","# =====================================\n","# 5 TRIADIC PLSI TRAINING\n","# =====================================\n","def train_triadic_plsi(train_triplets, U, D, W, K=5, max_iter=20, eps=1e-12):\n","    Pz_u = np.random.rand(U, K)\n","    Pz_u /= Pz_u.sum(axis=1, keepdims=True)\n","\n","    Pd_z = np.random.rand(K, D)\n","    Pd_z /= Pd_z.sum(axis=1, keepdims=True)\n","\n","    Pw_z = np.random.rand(K, W)\n","    Pw_z /= Pw_z.sum(axis=1, keepdims=True)\n","\n","    for it in range(max_iter):\n","        Q = []\n","        for (u, d, w, c) in train_triplets:\n","            val = Pz_u[u, :] * Pd_z[:, d] * Pw_z[:, w]\n","            s = val.sum()\n","            q = np.ones(K)/K if s < eps else val/s\n","            Q.append((u, d, w, c, q))\n","\n","        Pz_u_new = np.zeros_like(Pz_u)\n","        Pd_z_new = np.zeros_like(Pd_z)\n","        Pw_z_new = np.zeros_like(Pw_z)\n","\n","        for (u, d, w, c, q) in Q:\n","            Pz_u_new[u, :] += c * q\n","            Pd_z_new[:, d] += c * q\n","            Pw_z_new[:, w] += c * q\n","\n","        # Normalize\n","        Pz_u = (Pz_u_new.T / (Pz_u_new.sum(axis=1) + eps)).T\n","        Pd_z = (Pd_z_new.T / (Pd_z_new.sum(axis=1) + eps)).T\n","        Pw_z = (Pw_z_new.T / (Pw_z_new.sum(axis=1) + eps)).T\n","\n","        # Log-likelihood\n","        ll = 0.0\n","        for (u, d, w, c) in train_triplets:\n","            prob = np.sum(Pz_u[u, :] * Pd_z[:, d] * Pw_z[:, w])\n","            ll += c * np.log(prob + eps)\n","        print(f\"Iter {it+1}/{max_iter}, train log-likelihood = {ll:.2f}\")\n","\n","    return Pz_u, Pd_z, Pw_z\n","\n","K = 5\n","Pz_u, Pd_z, Pw_z = train_triadic_plsi(train_triplets, U, D, W, K=K, max_iter=15)\n","\n","# =====================================\n","# 6 PERPLEXITY & EVALUATION\n","# =====================================\n","def compute_perplexity(triplets, Pz_u, Pd_z, Pw_z, eps=1e-12):\n","    N = len(triplets)\n","    ll = 0.0\n","    for (u, d, w, c) in triplets:\n","        prob = np.sum(Pz_u[u, :] * Pd_z[:, d] * Pw_z[:, w])\n","        ll += c * np.log(prob + eps)\n","    perp = np.exp(-ll / N)\n","    return ll, perp\n","\n","ll_test, perp_test = compute_perplexity(test_triplets, Pz_u, Pd_z, Pw_z)\n","print(\"Triadic PLSI — Test log-likelihood = %.2f, Perplexity = %.2f\" % (ll_test, perp_test))\n","\n","# =====================================\n","# 7 TOP WORDS PER TOPIC\n","# =====================================\n","# Commented out the original topic printing\n","# top_n = 10\n","# for z in range(K):\n","#     top_w_idx = np.argsort(Pw_z[z, :])[-top_n:]\n","#     top_words = [vocab[w] for w in top_w_idx]\n","#     print(f\"Topic {z}: {top_words}\")\n","\n","# =====================================\n","# 8 BASELINE VANILLA PLSI (DOC–WORD)\n","# =====================================\n","dw_counts = defaultdict(int)\n","for (u, d, w, c) in train_triplets:\n","    dw_counts[(d, w)] += c\n","dw_triplets = [(d, w, c) for (d, w), c in dw_counts.items()]\n","\n","def train_plsi_doc_word(dw_triplets, D, W, K=5, max_iter=20, eps=1e-12):\n","    Pz_d = np.random.rand(D, K)\n","    Pz_d /= Pz_d.sum(axis=1, keepdims=True)\n","    Pw_z = np.random.rand(K, W)\n","    Pw_z /= Pw_z.sum(axis=1, keepdims=True)\n","\n","    for it in range(max_iter):\n","        Q = []\n","        for (d, w, c) in dw_triplets:\n","            val = Pz_d[d, :] * Pw_z[:, w]\n","            s = val.sum()\n","            q = np.ones(K)/K if s < eps else val/s\n","            Q.append((d, w, c, q))\n","\n","        Pz_d_new = np.zeros_like(Pz_d)\n","        Pw_z_new = np.zeros_like(Pw_z)\n","\n","        for (d, w, c, q) in Q:\n","            Pz_d_new[d, :] += c * q\n","            Pw_z_new[:, w] += c * q\n","\n","        Pz_d = (Pz_d_new.T / (Pz_d_new.sum(axis=1) + eps)).T\n","        Pw_z = (Pw_z_new.T / (Pw_z_new.sum(axis=1) + eps)).T\n","\n","        ll = 0.0\n","        for (d, w, c) in dw_triplets:\n","            prob = np.sum(Pz_d[d, :] * Pw_z[:, w])\n","            ll += c * np.log(prob + eps)\n","        print(f\"Vanilla PLSI iter {it+1}/{max_iter}, ll = {ll:.2f}\")\n","\n","    return Pz_d, Pw_z\n","\n","Pz_d_bw, Pw_z_bw = train_plsi_doc_word(dw_triplets, D, W, K=K, max_iter=15)\n","\n","# Evaluate on test\n","dw_test_counts = defaultdict(int)\n","for (u, d, w, c) in test_triplets:\n","    dw_test_counts[(d, w)] += c\n","dw_test_triplets = [(d, w, c) for (d, w), c in dw_test_counts.items()]\n","\n","ll_bw = 0.0\n","for (d, w, c) in dw_test_triplets:\n","    prob = np.sum(Pz_d_bw[d, :] * Pw_z_bw[:, w])\n","    ll_bw += c * np.log(prob + 1e-12)\n","perp_bw = np.exp(-ll_bw / len(dw_test_triplets))\n","print(\"Baseline (Vanilla PLSI) — Test Perplexity = %.2f\" % perp_bw)\n","\n","# Call the function to get and print unique top words\n","unique_top_words = get_unique_top_words(Pw_z, vocab, top_n=10)\n","print(\"\\nUnique Top Words per Topic (Triadic PLSI):\")\n","for z in range(K):\n","    print(f\"Topic {z}: {unique_top_words[z]}\")"],"metadata":{"id":"M6vhZ64twojZ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"/v2/external/notebooks/intro.ipynb","timestamp":1762694181720}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}