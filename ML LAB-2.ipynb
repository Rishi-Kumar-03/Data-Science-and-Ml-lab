{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uRoJUEs8xQNYUv0nj-Yv1tFWhh7xqmq6","timestamp":1757245745847}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"e4AL5rHNcpkc","executionInfo":{"status":"ok","timestamp":1762670572430,"user_tz":-330,"elapsed":5591,"user":{"displayName":"durgaprasanth","userId":"08399602778327095816"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","# Step 1: Load the dataset\n","from sklearn.datasets import load_iris\n","iris = load_iris()\n","\n","\n","X = iris.data\n","X = np.hstack([np.ones((X.shape[0], 1)), X])\n","y = iris.target\n","y = y.reshape(-1, 1)\n","\n","encoder = OneHotEncoder(sparse_output=False)\n","y = encoder.fit_transform(y)\n","\n","# Step 2: Split into train and test\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=33\n",")"]},{"cell_type":"code","source":["Number_Of_Classes = len(y_train[0])\n","Number_Of_Samples = len(X_train)\n","Number_Of_Features = len(X_train[0])\n","\n","W = np.random.rand(Number_Of_Classes, Number_Of_Features)\n","\n","# Learning rate\n","eta = 0.04\n","epochs = 100\n","\n","for epoch in range(epochs):\n","    # Compute scores: shape (N, C)\n","    scores = X_train @ W.T\n","\n","    # Softmax probabilities: shape (N, C)\n","    exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))  # stability\n","    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n","\n","    # Gradient: (C, F)\n","    gradient = (y_train - probs).T @ X_train\n","\n","    # Update weights (gradient ascent)\n","    W += eta * gradient / Number_Of_Samples\n","\n","    # (Optional) compute log-likelihood\n","    if epoch % 10 == 0:\n","        ll = np.sum(y_train * np.log(probs + 1e-9))\n","        print(f\"Epoch {epoch}, JLL: {ll:.4f}\")\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7LyvGLiyoFl8","outputId":"47394e98-b097-4661-d12c-a75ac913387d","executionInfo":{"status":"ok","timestamp":1762670572523,"user_tz":-330,"elapsed":87,"user":{"displayName":"durgaprasanth","userId":"08399602778327095816"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, JLL: -244.8971\n","Epoch 10, JLL: -80.1642\n","Epoch 20, JLL: -74.4063\n","Epoch 30, JLL: -70.1408\n","Epoch 40, JLL: -66.8069\n","Epoch 50, JLL: -64.0918\n","Epoch 60, JLL: -61.8089\n","Epoch 70, JLL: -59.8409\n","Epoch 80, JLL: -58.1102\n","Epoch 90, JLL: -56.5641\n"]}]},{"cell_type":"code","source":["y_pred = np.argmax(X_test @ W.T, axis=1)\n","y_true = np.argmax(y_test, axis=1)\n","\n","accuracy = np.mean(y_pred == y_true)\n","print(\"Accuracy:\", accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oj63DJLiq4WJ","outputId":"e105ff95-c5a6-4b64-f948-cd9a35f3f217","executionInfo":{"status":"ok","timestamp":1762670572633,"user_tz":-330,"elapsed":108,"user":{"displayName":"durgaprasanth","userId":"08399602778327095816"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9666666666666667\n"]}]}]}